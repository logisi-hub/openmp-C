# openmp-C
A glance at the Concurrency Achieved through the OpenMP API - C++ is the Language of Choicthe MPI concurrency construct (message passing interface), is conceptually similar to the OpenMP API. The success of OpenMP can be attributed to a number of factors. One is its strong emphasis on structured parallel programming. Another is that OpenMP is comparatively simple to use, since the burden of working out the details of the parallel program is up to the compiler. It has the major advantage of being widely adopted, so that an OpenMP application will run on many different platforms. But above all, OpenMP is timely. With the strong growth in deployment of both small and large SMPs and other multithreading hardware, the need for a shared-memory programming standard that is easy to learn and apply is accepted in the computing industry. This, in turn, means that all the major compilers--the Microsoft Visual C/C++ .NET for Windows and the GNU GCC compiler for Linux, and the Intel C/C++ compilers, for both Windows and Linux, also support OpenMP. But most programmers know that that any memory that can be shared when in a session on the Windows system will be shared. Memory allocation is a shared memory model. this holds true for loading an executable into runtime.The illusion that entire image has just be loaded is just that: an illusion. In reality\, only pieces ofc the executable image are loaded at a single time, but as they are needed. This is called "lazy allocation". But shared memory appears counter-intuitive, but that is how a 512 MB quantity of RAM appears on the terminal's box. But the hardware manager in the control unit of the CPU is able is able to alloicate a process address and have it top down chronologicall and sequential form. The result is a physcial quantity translated into a "page". One page is 4 KB, or 4096 bytes. This is how we read that there is 2 GB  of memory for user-land and 2 GB for kernel mode. The CPU (well, as of around 7 years ago, had a 32 bit memory address scheme. Thus 2 raised to the 32nd power swould be over 4 billion. Oh, 4 billion plus is dewey decimal base 10. 4 GB is base 2 and metric. So OK. How do threads of execution achieve concurrency without the danger of a race condition or a dsata-access conflict?
A shared memory model, by defintion, would indicate that all  threads have access to the same globally shared memory. Data can be shared or private. for instance, a user types in personal data into a notepad. those are private bytes, as they would have no data value to any thread needing to access data. Stated loosely, shared datas is accessible by all threads, but the private data can be accessed only by the threads that own it. This brief description should point to two things: data transfer is transparent to the owner, and synchronization takes place (as seen by how the <omp> blocks break up segmernts of the code body), but this synchronization is mostly implicit.

